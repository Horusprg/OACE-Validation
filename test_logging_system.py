#!/usr/bin/env python3
"""
Script de teste para verificar o sistema de logging aprimorado.
Executa um teste r√°pido para validar todas as funcionalidades implementadas.
"""

import sys
import os
import numpy as np
import json

# Adiciona o diret√≥rio raiz ao path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from utils.optimization_logger import OptimizationLogger
from utils.optimization_utils import OptimizationManager
from optimizers.pso_fixed import PSO

def test_basic_logging():
    """Testa funcionalidades b√°sicas do sistema de logging."""
    print("üß™ Testando funcionalidades b√°sicas...")
    
    try:
        # 1. Testar OptimizationLogger
        logger = OptimizationLogger(log_dir="results")
        
        # Configura√ß√£o de teste
        config = {
            "population_size": 3,
            "max_iter": 5,
            "lambda_param": 0.5,
            "test": True
        }
        
        # Iniciar experimento
        logger.start_experiment(config)
        assert logger.current_experiment is not None, "Experimento n√£o foi iniciado"
        print("   ‚úÖ Experimento iniciado com sucesso")
        
        # Simular algumas itera√ß√µes
        for i in range(3):
            population = np.random.rand(3, 2)
            fitness_values = np.random.rand(3)
            best_position = population[np.argmax(fitness_values)]
            best_fitness = np.max(fitness_values)
            
            metrics = {
                "top1_acc": np.random.uniform(0.8, 0.95),
                "total_params": np.random.randint(100000, 500000)
            }
            
            logger.log_iteration(
                iteration=i+1,
                phase="PSO",
                population=population,
                fitness_values=fitness_values,
                best_position=best_position,
                best_fitness=best_fitness,
                metrics=metrics,
                architecture_config={"model_type": "TestModel"}
            )
        
        print("   ‚úÖ Itera√ß√µes logadas com sucesso")
        
        # Verificar se arquivos foram criados na pasta results
        log_file = os.path.join("results", logger.current_experiment, "logs", f"{logger.current_experiment}_log.json")
        assert os.path.exists(log_file), "Arquivo de log n√£o foi criado"
        
        csv_file = os.path.join("results", logger.current_experiment, "csv", f"{logger.current_experiment}_avaliacoes.csv")
        assert os.path.exists(csv_file), "Arquivo CSV n√£o foi criado"
        
        print("   ‚úÖ Arquivos de log criados com sucesso")
        
        # Verificar estrutura do log
        with open(log_file, 'r') as f:
            log_data = json.load(f)
        
        assert "experiment_info" in log_data, "Informa√ß√µes do experimento n√£o encontradas"
        assert "iterations" in log_data, "Itera√ß√µes n√£o encontradas"
        assert len(log_data["iterations"]) == 3, "N√∫mero incorreto de itera√ß√µes"
        
        print("   ‚úÖ Estrutura do log v√°lida")
        
        return True
        
    except Exception as e:
        print(f"   ‚ùå Erro no teste b√°sico: {e}")
        return False

def test_pso_integration():
    """Testa integra√ß√£o com PSO."""
    print("üß™ Testando integra√ß√£o com PSO...")
    
    try:
        # Criar gerenciador
        manager = OptimizationManager(log_dir="results")
        
        # Configura√ß√£o
        config = {
            "population_size": 3,
            "max_iter": 3,
            "lambda_param": 0.5,
            "test": True
        }
        
        # Iniciar experimento
        manager.start_experiment(config)
        
        # Criar PSO
        optimizer = PSO(
            population_size=3,
            n_dim=2,
            max_iter=3,
            lower_bound=-1,
            upper_bound=1
        )
        
        # Fun√ß√£o de fitness simples
        def test_fitness(x):
            return -np.sum(x**2, axis=1)
        
        # Fun√ß√£o de m√©tricas simulada
        def test_metrics(x):
            return {
                "top1_acc": np.random.uniform(0.8, 0.95),
                "total_params": np.random.randint(100000, 500000),
                "avg_inference_time": np.random.uniform(0.01, 0.1)
            }
        
        # Executar otimiza√ß√£o
        best_position, best_fitness = manager.run_optimization(
            optimizer=optimizer,
            fitness_function=test_fitness,
            metrics_function=test_metrics
        )
        
        print("   ‚úÖ Otimiza√ß√£o PSO executada com sucesso")
        
        # Verificar resultados
        assert best_position is not None, "Melhor posi√ß√£o n√£o encontrada"
        assert best_fitness is not None, "Melhor fitness n√£o encontrado"
        
        print(f"   ‚úÖ Melhor posi√ß√£o: {best_position}")
        print(f"   ‚úÖ Melhor fitness: {best_fitness}")
        
        # Verificar arquivos gerados na pasta results
        log_file = os.path.join("results", manager.current_experiment, "logs", f"{manager.current_experiment}_log.json")
        csv_file = os.path.join("results", manager.current_experiment, "csv", f"{manager.current_experiment}_avaliacoes.csv")
        
        assert os.path.exists(log_file), f"Arquivo {log_file} n√£o encontrado"
        assert os.path.exists(csv_file), f"Arquivo {csv_file} n√£o encontrado"
        
        print("   ‚úÖ Arquivos de resultado criados")
        
        return True
        
    except Exception as e:
        print(f"   ‚ùå Erro no teste PSO: {e}")
        return False

def test_checkpoint_functionality():
    """Testa funcionalidades de checkpoint."""
    print("üß™ Testando funcionalidades de checkpoint...")
    
    try:
        # Criar gerenciador
        manager = OptimizationManager(log_dir="results")
        
        # Configura√ß√£o
        config = {
            "population_size": 2,
            "max_iter": 15,  # Aumentado para garantir checkpoints
            "lambda_param": 0.5,
            "test": True
        }
        
        # Iniciar experimento
        manager.start_experiment(config)
        
        # Criar PSO
        optimizer = PSO(
            population_size=2,
            n_dim=2,
            max_iter=15,  # Aumentado para garantir checkpoints
            lower_bound=-1,
            upper_bound=1
        )
        
        # Fun√ß√£o de fitness
        def test_fitness(x):
            return -np.sum(x**2, axis=1)
        
        # Fun√ß√£o de m√©tricas
        def test_metrics(x):
            return {
                "top1_acc": np.random.uniform(0.8, 0.95),
                "total_params": np.random.randint(100000, 500000)
            }
        
        # Executar otimiza√ß√£o
        best_position, best_fitness = manager.run_optimization(
            optimizer=optimizer,
            fitness_function=test_fitness,
            metrics_function=test_metrics
        )
        
        print("   ‚úÖ Otimiza√ß√£o executada com sucesso")
        
        # Verificar se checkpoints foram criados
        checkpoint_files = []
        for root, dirs, files in os.walk("results"):
            for file in files:
                if "checkpoint" in file:
                    checkpoint_files.append(os.path.join(root, file))
        
        # Verificar especificamente na pasta checkpoints do experimento
        checkpoints_dir = os.path.join("results", manager.current_experiment, "checkpoints")
        if os.path.exists(checkpoints_dir):
            checkpoint_files_in_dir = [f for f in os.listdir(checkpoints_dir) if "checkpoint" in f]
            print(f"   üìÅ Checkpoints encontrados em {checkpoints_dir}: {len(checkpoint_files_in_dir)}")
        
        assert len(checkpoint_files) > 0, "Nenhum checkpoint foi criado"
        
        print("   ‚úÖ Checkpoints criados com sucesso")
        
        return True
        
    except Exception as e:
        print(f"   ‚ùå Erro no teste de checkpoint: {e}")
        return False

def test_analysis_functions():
    """Testa fun√ß√µes de an√°lise de resultados."""
    print("üß™ Testando fun√ß√µes de an√°lise...")
    
    try:
        # Criar gerenciador
        manager = OptimizationManager(log_dir="results")
        
        # Configura√ß√£o
        config = {
            "population_size": 2,
            "max_iter": 3,
            "lambda_param": 0.5,
            "test": True
        }
        
        # Iniciar experimento
        manager.start_experiment(config)
        
        # Criar PSO
        optimizer = PSO(
            population_size=2,
            n_dim=2,
            max_iter=3,
            lower_bound=-1,
            upper_bound=1
        )
        
        # Fun√ß√£o de fitness
        def test_fitness(x):
            return -np.sum(x**2, axis=1)
        
        # Fun√ß√£o de m√©tricas
        def test_metrics(x):
            return {
                "top1_acc": np.random.uniform(0.8, 0.95),
                "total_params": np.random.randint(100000, 500000)
            }
        
        # Executar otimiza√ß√£o
        best_position, best_fitness = manager.run_optimization(
            optimizer=optimizer,
            fitness_function=test_fitness,
            metrics_function=test_metrics
        )
        
        print("   ‚úÖ Otimiza√ß√£o executada com sucesso")
        
        # Verificar se log foi criado
        log_file = os.path.join("results", manager.current_experiment, "logs", f"{manager.current_experiment}_log.json")
        assert os.path.exists(log_file), f"Log n√£o encontrado: {log_file}"
        
        # Carregar e analisar log
        with open(log_file, 'r') as f:
            log_data = json.load(f)
        
        # Verificar estrutura b√°sica
        assert "experiment_info" in log_data, "Informa√ß√µes do experimento n√£o encontradas"
        assert "iterations" in log_data, "Itera√ß√µes n√£o encontradas"
        assert "optimization_summary" in log_data, "Resumo da otimiza√ß√£o n√£o encontrado"
        
        # Verificar dados das itera√ß√µes
        iterations = log_data["iterations"]
        assert len(iterations) > 0, "Nenhuma itera√ß√£o encontrada"
        
        # Verificar se cada itera√ß√£o tem os campos necess√°rios
        for iteration in iterations:
            assert "iteration" in iteration, "N√∫mero da itera√ß√£o n√£o encontrado"
            assert "best_fitness" in iteration, "Melhor fitness n√£o encontrado"
            assert "best_position" in iteration, "Melhor posi√ß√£o n√£o encontrada"
        
        print("   ‚úÖ An√°lise de resultados executada com sucesso")
        
        return True
        
    except Exception as e:
        print(f"   ‚ùå Erro no teste de an√°lise: {e}")
        return False

def main():
    """Executa todos os testes."""
    print("üöÄ INICIANDO TESTES DO SISTEMA DE LOGGING APRIMORADO")
    print("=" * 60)
    
    tests = [
        ("Funcionalidades B√°sicas", test_basic_logging),
        ("Integra√ß√£o PSO", test_pso_integration),
        ("Checkpoints", test_checkpoint_functionality),
        ("An√°lise de Resultados", test_analysis_functions)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\nüìã Executando: {test_name}")
        try:
            if test_func():
                print(f"‚úÖ {test_name}: PASSOU")
                passed += 1
            else:
                print(f"‚ùå {test_name}: FALHOU")
        except Exception as e:
            print(f"‚ùå {test_name}: ERRO - {e}")
    
    print("\n" + "=" * 60)
    print(f"üìä RESULTADOS DOS TESTES")
    print(f"   ‚Ä¢ Total de testes: {total}")
    print(f"   ‚Ä¢ Testes aprovados: {passed}")
    print(f"   ‚Ä¢ Testes reprovados: {total - passed}")
    print(f"   ‚Ä¢ Taxa de sucesso: {(passed/total)*100:.1f}%")
    
    if passed == total:
        print("\nüéâ TODOS OS TESTES PASSARAM!")
        print("‚úÖ Sistema de logging est√° funcionando corretamente")
    else:
        print(f"\n‚ö†Ô∏è {total - passed} TESTE(S) FALHARAM")
        print("‚ùå Verifique os erros acima")
    
    print("\n" + "=" * 60)
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 